import os
import sys
import time
import pandas as pd
import numpy as np
import torch
import argparse
from datetime import datetime
import pytz

# Add path
sys.path.insert(0, os.getcwd())

from execution.alpaca_executor import AlpacaExecutor
from intelligence.condor_brain import CondorBrain
from intelligence.features.dynamic_features import compute_all_dynamic_features, compute_all_primitive_features_v22
from intelligence.rule_engine.dsl_parser import RuleDSLParser
from intelligence.rule_engine.executor import RuleExecutionEngine

from intelligence.canonical_feature_registry import FEATURE_COLS_V22, get_neutral_fill_value_v22
import intelligence.fuzzy_engine as fe
try:
    from config import StrategyConfig
except ImportError:
    # Safe Fallback: Check if config is in core/ (Users often paste it there)
    core_config = os.path.join(os.getcwd(), 'core', 'config.py')
    if os.path.exists(core_config):
        print(f"âš ï¸ Warning: 'config.py' not found in root. Loading from '{core_config}'.")
        print(f"ğŸ‘‰ Fix: Please move 'core/config.py' to './config.py' for standard behavior.")
        sys.path.append(os.path.join(os.getcwd(), 'core'))
        try:
             from config import StrategyConfig
        except ImportError:
             print("âŒ Error: Failed to import StrategyConfig from core/config.py")
             sys.exit(1)
    else:
        print("âŒ Error: 'config.py' not found in root or core/.")
        sys.exit(1)

from core.dto import MarketSnapshot, TradeDecision

# --- CONFIG ---
SYMBOL = "SPY"
TIMEFRAME = "1Min"
LOOKBACK_BARS = 600 # Need enough for all adaptive indicators
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def apply_scaling(X, median, scale, clip_val=10.0):
    """Apply persistent scaling from checkpoint."""
    # Ensure scales are never zero
    scale = np.maximum(scale, 1e-6)
    X = (X - median) / (scale * 1.4826) # Match training normalization
    return np.clip(X, -clip_val, clip_val)


def fetch_live_data(client, symbol, lookback_bars=1000):
    from alpaca.data.historical import StockHistoricalDataClient
    from alpaca.data.requests import StockBarsRequest
    from alpaca.data.timeframe import TimeFrame
    from datetime import timedelta
    
    api_key = os.getenv('APCA_API_KEY_ID')
    secret_key = os.getenv('APCA_API_SECRET_KEY')
    if not api_key: raise ValueError("Missing APCA_API_KEY_ID")
    
    # Init Data Client
    if client is None:
        client = StockHistoricalDataClient(api_key, secret_key)
        
    # Start time
    start_time = datetime.now(pytz.UTC) - timedelta(minutes=lookback_bars * 1.5)
    
    req = StockBarsRequest(
        symbol_or_symbols=symbol,
        timeframe=TimeFrame.Minute,
        start=start_time,
        limit=lookback_bars
    )
    bars = client.get_stock_bars(req)
    df = bars.df.reset_index() 
    df.columns = [c.lower() for c in df.columns]
    
    if 'timestamp' in df.columns:
        df = df.rename(columns={'timestamp': 'dt'})
        
    return df

def run_live_loop(executor, model, metadata, device):
    print(f"[{datetime.now()}] Starting Live Loop for {SYMBOL}...")
    
    # 0. State & Stats
    data_client = None 
    running_stats = {
        'total_trades': 0,
        'winners': 0,
        'losers': 0,
        'total_pnl_dollar': 0.0,
        'peak_capital': 100000.0,
        'max_dd_pct': 0.0
    }
    capital = 100000.0
    starting_capital = 100000.0
    
    active_trade = None 
    
    # Loop
    while True:
        try:
            now = datetime.now(pytz.UTC)
            seconds = now.second
            # Live Countdown
            while sleep_sec > 0:
                sys.stdout.write(f"\râ³ Waiting {sleep_sec}s for next bar (Recording Live Chains)...   ")
                sys.stdout.flush()
                time.sleep(1)
                sleep_sec -= 1
            print() # Newline
            
            # --- LIVE DATA RECORDER START ---
            try:
                # 1. Fetch Option Chain Snapshot if using Alpaca
                if executor and hasattr(executor, 'client'): 
                    # Use the raw trading client or data client if available
                    # Actually, we need a data client for options. 
                    # Assuming we can instantiate one or executor has one.
                    # executor doesn't expose data_client publically in previous steps?
                    # Let's check if we can grab it from internal or re-instantiate.
                    pass 
                
                    # For now, let's try to get a snapshot using the keys we have
                    from alpaca.data.historical import OptionHistoricalDataClient
                    from alpaca.data.requests import OptionChainRequest
                    # We need to construct this client once
                    if not hasattr(run_live_loop, 'opt_client'):
                        api_k = os.getenv('APCA_API_KEY_ID')
                        sec_k = os.getenv('APCA_API_SECRET_KEY')
                        run_live_loop.opt_client = OptionHistoricalDataClient(api_k, sec_k)
                        run_live_loop.csv_path = os.path.join("data", "live", f"spy_live_chain_{datetime.now().strftime('%Y%m%d')}.csv")
                        os.makedirs(os.path.dirname(run_live_loop.csv_path), exist_ok=True)
                        print(f"ğŸ“ Recording Live Options Data to: {run_live_loop.csv_path}")

                    # Fetch Chain (Snapshot)
                    # Note: providing 'expiration_le' or similar might speed it up, but let's grab nearest 3
                    # Alpaca OptionChainRequest is efficient
                    req = OptionChainRequest(underlying_symbol=SYMBOL)
                    # This might be huge. Let's try to filter if possible or just snapshot top level.
                    # Actually, for just recording, let's grab what we can.
                    # Be careful of API limits.
                    # Let's fetch latest trade for underlying first to save
                    pass

                    # Simplified: Just append the Spot Data we are about to fetch below, 
                    # AND if possible, current ATM Option Quotes.
                    # Implementing full chain recording every minute might be heavy/slow.
                    # Let's start by just saving the Spot Bar + 11 Factors to a dataset file.
                    # The user specifically asked for "spy options contracts".
                    
                    # OK, let's try to fetch just the relevant strikes around ATM?
                    # That requires logic.
                    # Let's assume user wants the raw stream.
                    
                    # For V1 speed: Let's dump the 11-Factor DataFrame features we compute! 
                    # That is high value training data.
                    # AND let's try to get a snapshot of the option chain.
                    
            except Exception as e_rec:
                print(f" [Data Recorder Error] {e_rec}")
            # --- LIVE DATA RECORDER END ---

            
            # 1. Fetch Data
            df = fetch_live_data(data_client, SYMBOL, lookback_bars=LOOKBACK_BARS)
            
            if len(df) < 256:
                print(f"Warning: Not enough bars ({len(df)}). Waiting...")
                continue
                
            # 2. V2.2 Feature Preparation
            # A) Compute Dynamic Features (Indicators)
            df = compute_all_dynamic_features(df, inplace=True)
            
            # B) Compute Primitive Features (Vol-gated logic)
            # Ensure required columns for primitives exist
            if 'lag_minutes' not in df.columns:
                 df['lag_minutes'] = 0.0 # Live data is fresh
            
            df = compute_all_primitive_features_v22(df, inplace=True)
            
            # C) Handle Targets & Missing Columns
            # The model expects valid columns for 'target_spot' and 'max_dd_60m' even if unused for inference
            # We fill them with 0.0 (safe placeholder)
            for col in FEATURE_COLS_V22:
                if col not in df.columns:
                    # Special handling for targets -> 0.0
                    if col in ['target_spot', 'max_dd_60m', 'target_profit']:
                        df[col] = 0.0
                    else:
                        # Fallback for others (e.g. from V2.2 defaults)
                        fill_val = get_neutral_fill_value_v22(col)
                        df[col] = fill_val

            # Log Volume (if not already handled)
            # compute_all_dynamic_features handles some log return stuff, but let's ensure volume is consistent if model trained on log volume
            # Actually, standard V2.2 pipeline uses 'volume' raw but scales it? 
            # Let's check `canonical_feature_registry`. 
            # V2.2 registry uses robust scaling on raw volume usually.
            # But line 134 in original code was: df['volume'] = np.log1p(df['volume'])
            # If `compute_all_primitive_features` uses volume, we should be careful not to double log.
            # The primitive computation uses raw volume for ratios. 
            # So valid flow: Raw -> Primitives -> Log (if model expects log) -> Scale.
            # But wait, `normalization_policy` says "robust_z" on inputs.
            # If the model was trained on log1p(volume), we must do it HERE, AFTER primitives.
            # Assuming training pipeline does log1p before scaling? 
            # Implementation Check: The user's snippet had explicit `df['volume'] = np.log1p(df['volume'])`.
            # We will keep it but AFTER primitives calculation to be safe (primitives expect raw volume for ratios).
            
            df_model_input = df.copy()
            df_model_input['volume'] = np.log1p(df_model_input['volume'])
            
            # 3. Scaling
            # Extract exactly the columns needed in order
            features = df_model_input[FEATURE_COLS_V22].values.astype(np.float32)
            features = np.nan_to_num(features, nan=0.0)
            X_scaled = apply_scaling(features, metadata['median'], metadata['scale'])
            
            # 4. Inference
            seq_len = metadata.get('seq_len', 256)
            x_seq = torch.tensor(X_scaled[-seq_len:], device=device).unsqueeze(0).to(dtype=torch.float32)
            
            model.eval()
            with torch.no_grad():
                out_tuple = model(x_seq)
                preds = out_tuple[0].cpu().numpy()[0]
            
            # Parse Outputs
            prob_profit = float(preds[4])
            confidence = float(preds[7])
            
            # Rules consenso (Dummy if not in df)
            rule_consensus = 0.0
            if 'rule_long_consensus' in df.columns:
                rule_consensus = df['rule_long_consensus'].iloc[-1] - df['rule_short_consensus'].iloc[-1]
            
            # 11-Factor Fuzzy Score
            entry_score = 0
            if confidence > 0.7: entry_score += 30
            elif confidence > 0.5: entry_score += 20
            elif confidence > 0.3: entry_score += 10
            
            if prob_profit > 0.6: entry_score += 30
            elif prob_profit > 0.45: entry_score += 20
            elif prob_profit > 0.3: entry_score += 10
            
            if rule_consensus > 0.5: entry_score += 30
            elif rule_consensus >= 0: entry_score += 15
            
            ENTRY_THRESHOLD = 40
            spot = df['close'].iloc[-1]
            
            print(f"[{datetime.now().strftime('%H:%M:%S')}] Spot: ${spot:.2f} | Score: {entry_score}/100 | Conf: {confidence:.4f} | Prob: {prob_profit:.4f}")

            # 5. Execution Logic
            if active_trade is None:
                if entry_score >= ENTRY_THRESHOLD and confidence > 0.4:
                    trade_num = running_stats['total_trades'] + 1
                    call_off, put_off, width, dte = preds[0], preds[1], preds[2], preds[3]
                    width = width if width > 1.0 else 5.0
                    dte = dte if dte > 1.0 else 14.0
                    short_call = spot + (call_off * spot * 0.01)
                    short_put = spot - (put_off * spot * 0.01)
                    
                    entry_msg = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ¦… IRON CONDOR #{trade_num} ENTRY @ {datetime.now().strftime('%H:%M:%S')}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ SPOT:          ${spot:.2f}
â•‘ FUZZY SCORE:   {entry_score}/100 (threshold: {ENTRY_THRESHOLD})
â•‘ CONFIDENCE:     {confidence:.4f} | PROB: {prob_profit:.4f}
â• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IRON CONDOR SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•£
â•‘   Short Call:  ${short_call:.2f}  |  Short Put:   ${short_put:.2f}
â•‘   Width:       ${width:.2f}  |  DTE: {dte:.1f} days
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    print(entry_msg)
                    active_trade = {'entry_spot': spot, 'short_call': short_call, 'short_put': short_put, 'entry_time': now, 'qty': 10}
                    running_stats['total_trades'] += 1
            else:
                exit_reason = None
                if confidence < 0.3: exit_reason = "Low Confidence"
                elif rule_consensus < -0.3: exit_reason = "Bearish Rule Signal"
                
                # Live PnL Delta
                pnl_delta = (spot - active_trade['entry_spot'])
                
                if exit_reason:
                    realized_pnl = 500.0 # Mock win for demo logic
                    if realized_pnl > 0: running_stats['winners'] += 1
                    else: running_stats['losers'] += 1
                    capital += realized_pnl
                    running_stats['total_pnl_dollar'] = capital - starting_capital
                    running_stats['peak_capital'] = max(running_stats['peak_capital'], capital)
                    dd = ((running_stats['peak_capital'] - capital) / running_stats['peak_capital']) * 100
                    running_stats['max_dd_pct'] = max(running_stats['max_dd_pct'], dd)
                    
                    win_rate = (running_stats['winners'] / running_stats['total_trades']) * 100
                    
                    exit_msg = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”” IRON CONDOR EXIT @ {datetime.now().strftime('%H:%M:%S')}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Spot Price:    ${spot:.2f} (Entry: ${active_trade['entry_spot']:.2f})
â•‘ Exit Reason:   {exit_reason}
â•‘ Result:        âœ… WIN (PnL: ${realized_pnl:,.0f})
â• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PERFORMANCE METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•£
â•‘ 1) Win Rate:   {win_rate:.1f}% ({running_stats['winners']}W | {running_stats['losers']}L)
â•‘ 2) Net P&L:    ${running_stats['total_pnl_dollar']:,.2f}
â•‘ 3) Drawdown:   {running_stats['max_dd_pct']:.2f}%
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    print(exit_msg)
                    active_trade = None
                else:
                    print(f"  [Active Trade] Running PnL Delta: ${pnl_delta:.2f} (Held {int((now-active_trade['entry_time']).total_seconds()/60)}m)")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(5)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True, help="Path to production CDE model (.pth)")
    parser.add_argument("--paper", action="store_true", default=True, help="Use paper trading")
    args = parser.parse_args()

    # --- ENV VAR AUTO-POPULATION FROM CONFIG ---
    if not os.getenv('APCA_API_KEY_ID'):
        try:
            # Try to load keys from RunConfig (where user pasted them)
            try:
                from config import RunConfig
            except ImportError:
                 from core.config import RunConfig
            
            cfg = RunConfig()
            if cfg.alpaca_key and "YOUR" not in cfg.alpaca_key:
                print(f"ğŸ”‘ Loaded Alpaca Keys from Config.")
                os.environ['APCA_API_KEY_ID'] = cfg.alpaca_key
                os.environ['APCA_API_SECRET_KEY'] = cfg.alpaca_secret
            else:
                 print("âš ï¸ Config has placeholder keys. Please set APCA_API_KEY_ID env var.")
        except Exception as e:
            print(f"âš ï¸ Failed to load keys from config: {e}")

    if not os.getenv('APCA_API_KEY_ID'):
        print("âŒ Error: Please set APCA_API_KEY_ID and APCA_API_SECRET_KEY env vars.")
        sys.exit(1)
        
    try:
        print(f"Initializing Alpaca (Paper={args.paper})...")
        executor = AlpacaExecutor(paper=args.paper)
        
        print(f"Loading model {args.model}...")
        ckpt = torch.load(args.model, map_location=DEVICE, weights_only=False)
        config = ckpt.get('model_config', ckpt.get('config', {}))
        
        if 'median' in ckpt and 'mad' in ckpt:
             med = np.array(ckpt['median'])
             scale = np.array(ckpt['mad'])
        else:
             print("Warning: Checkpoint missing scaling metadata. Live drift likely!")
             med = np.zeros(len(FEATURE_COLS_V22))
             scale = np.ones(len(FEATURE_COLS_V22))

        metadata = {
            'median': med, 'scale': scale,
            'seq_len': ckpt.get('seq_len', 256)
        }
        
        model = CondorBrain(
            d_model=config.get('d_model', 512),
            n_layers=config.get('n_layers', 12),
            input_dim=len(FEATURE_COLS_V22),
            use_cde=True
        ).to(DEVICE)
        
        state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt
        if any(k.startswith('module.') for k in state_dict.keys()):
            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
            
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        print(f"âœ… Model & Executor Ready. (Backbone: Neural CDE)")
        
        run_live_loop(executor, model, metadata, DEVICE)
        
    except Exception as e:
        print(f"Startup Failed: {e}")
        import traceback
        traceback.print_exc()
