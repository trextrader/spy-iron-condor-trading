# ============================================================
# CONDORBRAIN RETRAINING V2 - FIXING POSTERIOR COLLAPSE
# ============================================================
print("ðŸš€ Starting CondorBrain Retraining V2 (Shock Therapy)...")
'''
# --- 0. PREP & CLEAN ---
import os
print("ðŸ”„ Syncing Repo...")
os.system("cd spy-iron-condor-trading && git fetch origin && git reset --hard origin/main")
print("âœ… Repo synced")
'''
import sys
sys.path.insert(0, '/kaggle/working/spy-iron-condor-trading')
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
from torch.utils.tensorboard import SummaryWriter
from intelligence.condor_brain import CondorBrain
from intelligence.canonical_feature_registry import (
    FEATURE_COLS_V21, INPUT_DIM_V21, VERSION_V21,
    NAN_POLICY_V21, NORMALIZATION_POLICY_V21,
    apply_semantic_nan_fill,
)
from intelligence.features.dynamic_features import compute_all_dynamic_features
from intelligence.training_monitor import (
    TrainingMonitor, compute_val_head_losses, sample_predictions, MAIN_HEADS
)
import io
from PIL import Image
import matplotlib.pyplot as plt

# --- TRAINING CONFIG ---
# ROWS_TO_LOAD: Number of rows from end of dataset
#   100K rows â‰ˆ 1K unique spot bars (~1 min compute)
#   300K rows â‰ˆ 3K unique spot bars (~2 min compute)
#   1M rows â‰ˆ 10K unique spot bars (~3 min compute)
#   3M rows â‰ˆ 30K unique spot bars (~5 min compute)
#   Full dataset: ~10M rows â‰ˆ 100K unique spot bars

ROWS_TO_LOAD = 100_000  # âš¡ CHANGE THIS TO SCALE TRAINING âš¡
EPOCHS = 2              # Quick test: 2, Full training: 10

# Derived estimate
estimated_spots = max(ROWS_TO_LOAD // 100, 100)  # ~100 options per spot bar
print(f"ðŸ“Š Config: {ROWS_TO_LOAD:,} rows â†’ ~{estimated_spots:,} unique spot bars, {EPOCHS} epochs")

BATCH_SIZE = 128
LR = 5e-4
SEQ_LEN = 256
PREDICT_HORIZON = 32

# Optimization Flags
DIFFUSION_WARMUP_EPOCHS = 1  # Skip diffusion for first epoch
DIFFUSION_STEPS_TRAIN = 50

device = torch.device('cuda')
n_gpus = torch.cuda.device_count()
USE_DATAPARALLEL = (n_gpus > 1)  # Auto-detect: Kaggle dual T4 vs Colab single T4
print(f"   GPU: {torch.cuda.get_device_name(0)} x{n_gpus} (DataParallel={'ON' if USE_DATAPARALLEL else 'OFF'})")

# --- LAUNCH TENSORBOARD (Colab/Jupyter Only) ---
try:
    from IPython import get_ipython
    ip = get_ipython()
    if ip:
        print("ðŸ“Š Launching TensorBoard inline...")
        # Kill any existing TensorBoard process on port 6006
        try:
            # Use fuser to kill process on TCP port 6006 (Linux/Colab)
            ip.getoutput("fuser -k 6006/tcp")
        except:
            pass
            
        # Reload extension to be safe
        ip.run_line_magic('reload_ext', 'tensorboard') 
        ip.run_line_magic('tensorboard', '--logdir=runs/condor_brain --port=6006')
except Exception as e:
    # Not running in IPython or other error
    pass

# --- 1. DATA LOADING & PREP ---
print(f"\n[1/4] Loading & Processing {ROWS_TO_LOAD:,} Rows...")

# Auto-detect environment: Kaggle vs Colab vs Local
KAGGLE_PATH = "/kaggle/input/spy-options-data/mamba_institutional_1m.csv"
COLAB_PATH = "/content/spy-iron-condor-trading/data/processed/mamba_institutional_1m.csv"
COLAB_ROOT_PATH = "data/processed/mamba_institutional_1m.csv"  # If already in repo root
LOCAL_PATH = "data/processed/mamba_institutional_1m.csv"

for p in [KAGGLE_PATH, COLAB_PATH, COLAB_ROOT_PATH, LOCAL_PATH]:
    if os.path.exists(p):
        DATA_PATH = p
        break
else:
    raise FileNotFoundError(f"Data file not found!")

print(f"   Data: {DATA_PATH}")
df = pd.read_csv(DATA_PATH).iloc[-ROWS_TO_LOAD:]
print(f"   Shape: {df.shape}")

# V2.1 FEATURE SCHEMA (schema-driven feature count; dynamic indicators included)
FEATURE_COLS = FEATURE_COLS_V21
INPUT_DIM = INPUT_DIM_V21
print(f"   Using V2.1 Schema: {len(FEATURE_COLS)} features")
assert len(FEATURE_COLS) == INPUT_DIM, (
    f"Schema mismatch: len(FEATURE_COLS)={len(FEATURE_COLS)} != INPUT_DIM={INPUT_DIM}"
)

# ------------------------------
# FEATURE & TARGET CONSTRUCTION
# ------------------------------
eps = 1e-8

close = df["close"].to_numpy(np.float32)
highs = df["high"].to_numpy(np.float32)
lows = df["low"].to_numpy(np.float32)
opens = df["open"].to_numpy(np.float32)
volumes = df["volume"].to_numpy(np.float32)

log_c = np.log(close + eps)
log_v = np.log(volumes + 1.0)

r = np.zeros_like(close, dtype=np.float32)
r[1:] = np.diff(log_c)
rho = np.log((highs + eps) / (lows + eps)).astype(np.float32)
d = np.log((close + eps) / (opens + eps)).astype(np.float32)

v = np.zeros_like(volumes, dtype=np.float32)
v[1:] = np.diff(log_v).astype(np.float32)

Y_feat_np = np.stack([r, rho, d, v], axis=1).astype(np.float32)
# Sanitize targets (prevent NaNs and Ints)
Y_feat_np = np.nan_to_num(Y_feat_np, nan=0.0, posinf=5.0, neginf=-5.0)
Y_feat_np = np.clip(Y_feat_np, -10.0, 10.0)

# =========================================================================
# CRITICAL: Compute dynamic features on UNIQUE SPOT bars, then merge back.
# (Options data has ~100 rows per timestamp with same OHLCV)
# =========================================================================
print("   Extracting unique spot bars...")
ohlcv_cols = ['open', 'high', 'low', 'close', 'volume']
spot_key_cols = ['symbol', 'dt'] if 'dt' in df.columns else ['symbol']
if 'dt' not in df.columns and 'timestamp' in df.columns:
    spot_key_cols = ['symbol', 'timestamp']

# Find actual datetime column
dt_col = None
for c in ['dt', 'timestamp', 'datetime', 'date']:
    if c in df.columns:
        dt_col = c
        spot_key_cols = ['symbol', c] if 'symbol' in df.columns else [c]
        break

if dt_col is None:
    # No datetime column - use row order (rare case)
    print("   âš ï¸ No datetime column found, computing on all rows...")
    df = compute_all_dynamic_features(df, close_col="close", high_col="high", low_col="low")
else:
    # Extract unique spots
    spot_df = df.drop_duplicates(subset=spot_key_cols)[spot_key_cols + ohlcv_cols].copy()
    spot_df = spot_df.sort_values(spot_key_cols).reset_index(drop=True)
    n_unique = len(spot_df)
    print(f"   Unique spot bars: {n_unique:,} (from {len(df):,} options rows)")
    
    # Compute dynamic features on spots only
    print("   Computing dynamic features on spot bars...")
    spot_df = compute_all_dynamic_features(spot_df, close_col="close", high_col="high", low_col="low")
    
    # Get dynamic columns and merge back
    dynamic_cols = [c for c in spot_df.columns if c not in spot_key_cols + ohlcv_cols]
    print(f"   Dynamic columns: {len(dynamic_cols)}")
    merge_cols = spot_key_cols + dynamic_cols
    df = df.merge(spot_df[merge_cols], on=spot_key_cols, how='left')

# X features (schema-driven columns)
X_np = df[FEATURE_COLS].values.astype(np.float32)
# Apply per-feature semantic NaN filling (not global 0.0)
X_np = apply_semantic_nan_fill(X_np, FEATURE_COLS)

# ------------------------------
# TRAIN / VAL SPLIT
# ------------------------------
split_idx = int(len(X_np) * 0.9)
X_train_np = X_np[:split_idx]
X_val_np   = X_np[split_idx:]
Y_feat_train_np = Y_feat_np[:split_idx]
Y_feat_val_np   = Y_feat_np[split_idx:]

# ------------------------------
# SCALING (FIT TRAIN ONLY)
# ------------------------------
median = np.median(X_train_np, axis=0, keepdims=True).astype(np.float32)
mad = (np.median(np.abs(X_train_np - median), axis=0, keepdims=True) + 1e-8).astype(np.float32)

def robust_norm(x):
    return np.clip((x - median) / (1.4826 * mad), -10.0, 10.0).astype(np.float32)

X_train_np = robust_norm(X_train_np)
X_val_np   = robust_norm(X_val_np)

# POLICY TARGETS (NO WRAPAROUND)
# ------------------------------
future_close = np.empty_like(close)
future_close[:-60] = close[60:]
future_close[-60:] = np.nan
returns_60m = (future_close - close).astype(np.float32)
returns_60m = np.nan_to_num(returns_60m, nan=0.0)

vol_60m = (
    pd.Series(returns_60m)
    .rolling(60, min_periods=1)
    .std(ddof=0)
    .fillna(0.0)
    .to_numpy(np.float32)
)

Y_policy_np = np.zeros((len(X_np), 8), dtype=np.float32)
Y_policy_np[:, 0] = 2.0 + np.clip(returns_60m * 0.5, -1.0, 1.0) # Call Offset
Y_policy_np[:, 1] = 2.0 - np.clip(returns_60m * 0.5, -1.0, 1.0) # Put Offset
Y_policy_np[:, 2] = 5.0 + np.clip(vol_60m * 20, 0, 5)       # Width
Y_policy_np[:, 4] = 0.5 + np.clip(returns_60m * 0.1, -0.4, 0.4) # Prob Profit
Y_policy_np[:, 7] = 0.3 + np.clip(np.abs(returns_60m), 0.0, 0.6)  # Confidence

Y_policy_train_np = Y_policy_np[:split_idx]
Y_policy_val_np = Y_policy_np[split_idx:]

# ------------------------------
# FAST TORCH DATASET (NO TENSOR CREATION PER SAMPLE)
# ------------------------------
class SequenceDataset(Dataset):
    def __init__(self, X, Y_policy, Y_feat, seq_len, horizon=32):
        self.X = torch.from_numpy(X)
        self.Y_policy = torch.from_numpy(Y_policy)
        self.Y_feat = torch.from_numpy(Y_feat)
        self.seq_len = seq_len
        self.horizon = horizon
        self.max_i = len(X) - seq_len - horizon - 1
        
    def __len__(self):
        return self.max_i
        
    def __getitem__(self, idx):
        x_seq = self.X[idx : idx + self.seq_len]
        y_pol = self.Y_policy[idx + self.seq_len - 1]
        y_next = self.Y_feat[idx + self.seq_len]
        y_traj = self.Y_feat[idx + self.seq_len : idx + self.seq_len + self.horizon]
        return x_seq, y_pol, y_next, y_traj

train_dataset = SequenceDataset(X_train_np, Y_policy_train_np, Y_feat_train_np, SEQ_LEN, PREDICT_HORIZON)
val_dataset = SequenceDataset(X_val_np, Y_policy_val_np, Y_feat_val_np, SEQ_LEN, PREDICT_HORIZON)

num_workers = min(8, max(2, (os.cpu_count() or 4) // 2))

train_loader = DataLoader(
    train_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=True, 
    num_workers=num_workers, 
    pin_memory=True,
    persistent_workers=True, 
    prefetch_factor=4
)
val_loader = DataLoader(
    val_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=False, 
    num_workers=num_workers, 
    pin_memory=True,
    persistent_workers=True, 
    prefetch_factor=4
)

# --- 2. MODEL SETUP ---
print("\n[2/4] Initializing CondorBrain V2...")
model = CondorBrain(
    d_model=512,
    n_layers=12,
    input_dim=INPUT_DIM,  # V2.1: schema-driven feature count
    use_vol_gated_attn=True,
    use_topk_moe=True,
    moe_n_experts=3, moe_k=1,
    use_diffusion=True,
    diffusion_steps=DIFFUSION_STEPS_TRAIN
).to(device)

# --- INITIALIZATION FIX ---
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
             m.bias.data.fill_(0.01) # Small bias to prevent dead neurons

print("   Applying Xavier Initialization...")
model.apply(init_weights)

if USE_DATAPARALLEL and torch.cuda.device_count() > 1:
    print(f"   Using {torch.cuda.device_count()} GPUs (DataParallel)")
    model = nn.DataParallel(model)
else:
    print("   Using Single GPU (USE_DATAPARALLEL=False)")

optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)

# Scheduler: Cosine Annealing with Warmup/Restarts
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer, 
    max_lr=LR, 
    steps_per_epoch=len(train_loader), 
    epochs=EPOCHS,
    pct_start=0.1
)

criterion_policy = nn.MSELoss()
criterion_forecast = nn.HuberLoss()

# --- 3. TRAINING LOOP ---
print("\n[3/4] Retraining Loop (Balanced Loss)...")
scaler = torch.amp.GradScaler('cuda')

# Initialize TensorBoard Writer
writer = SummaryWriter(log_dir="runs/condor_brain")

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    total_pol_loss = 0
    total_feat_loss = 0
    total_diff_loss = 0
    
    # ðŸŒŸ Staged Training Logic ðŸŒŸ
    use_diffusion = (epoch >= DIFFUSION_WARMUP_EPOCHS)
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Diff={'ON' if use_diffusion else 'OFF'}]")
    
    for batch_idx, (x_seq, y_pol, y_next, y_traj) in enumerate(pbar):
        x_seq = x_seq.to(device, non_blocking=True)
        y_pol = y_pol.to(device, non_blocking=True)
        y_next = y_next.to(device, non_blocking=True)
        y_traj = y_traj.to(device, non_blocking=True) # (B, 32, 4)
        
        optimizer.zero_grad(set_to_none=True)
        
        with torch.amp.autocast('cuda'):
            # Only pass diffusion target if we are past warmup
            traj_target = y_traj if use_diffusion else None
            
            # Pass diffusion_target (y_traj) to compute diffusion loss internally
            # return_features=True -> output index 3
            # use_diffusion=True -> output index 4
            res = model(
                x_seq, 
                return_features=True, 
                diffusion_target=traj_target
            )
            
            # Unpack results
            # res structure: [outputs, regime, horizon, features, diffusion, experts...]
            outputs = res[0]
            # regime = res[1]
            # horizon = res[2]
            feat_pred = res[3]  # Index 3 if return_features=True
            diff_loss_scalar = res[4] if use_diffusion else None # Index 4 if use_diffusion=True
            
            # 1. Policy Loss
            loss_pol = criterion_policy(outputs, y_pol)
            
            # 2. Feature Loss (Next Step)
            loss_feat = criterion_forecast(feat_pred, y_next) * 1000.0

            # 3. Diffusion Loss
            loss_diff = torch.tensor(0.0, device=device)
            if diff_loss_scalar is not None:
                # Handle DataParallel: If gathered loss is a vector (one per GPU), take mean
                if diff_loss_scalar.dim() > 0:
                    diff_loss_scalar = diff_loss_scalar.mean()
                loss_diff = diff_loss_scalar * 10.0 # Scale diffusion loss
            
            # Total Loss
            loss = (loss_pol * 2.0) + loss_feat + loss_diff 
        
        # Stability check
        if not torch.isfinite(loss):
             pbar.write(f"âš ï¸  Non-finite loss encountered at batch {batch_idx}, skipping...")
             optimizer.zero_grad(set_to_none=True)
             continue

        scaler.scale(loss).backward()
        nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        
        total_loss += loss.item()
        total_pol_loss += loss_pol.item()
        total_feat_loss += loss_feat.item()
        total_diff_loss += loss_diff.item()
        
        pbar.set_postfix({
            'L_pol': f"{loss_pol.item():.4f}", 
            'L_feat': f"{loss_feat.item():.4f}",
            'L_diff': f"{loss_diff.item():.4f}"
        })
        
        # Periodic Logging (every 100 batches)
        if batch_idx % 100 == 0:
            pbar.write(f"   [Batch {batch_idx}] L_All={loss.item():.4f} | Pol={loss_pol.item():.4f} | Diff={loss_diff.item():.4f}")
            
        # TensorBoard Logging (every batch)
        global_step = epoch * len(train_loader) + batch_idx
        writer.add_scalar('Loss/Total', loss.item(), global_step)
        writer.add_scalar('Loss/Policy', loss_pol.item(), global_step)
        writer.add_scalar('Loss/Feature', loss_feat.item(), global_step)
        writer.add_scalar('Loss/Diffusion', loss_diff.item(), global_step)
        writer.add_scalar('LR', scheduler.get_last_lr()[0], global_step)
    
    avg_pol = total_pol_loss / len(train_loader)
    avg_feat = total_feat_loss / len(train_loader)
    avg_diff = total_diff_loss / len(train_loader)
    
    print(f"   Epoch {epoch+1} Train: Policy={avg_pol:.4f} | Feat={avg_feat:.4f} | Diff={avg_diff:.4f}")
    
    # TensorBoard Epoch Averages
    writer.add_scalar('Epoch/Policy_Loss', avg_pol, epoch+1)
    writer.add_scalar('Epoch/Feature_Loss', avg_feat, epoch+1)
    writer.add_scalar('Epoch/Diffusion_Loss', avg_diff, epoch+1)
    
    # === MONITOR & TENSORBOARD VISUALIZATION ===
    # This block replicates the rich logging from intelligence/train_condor_brain.py
    
    # 1. Compute per-head val losses (fast GPU accum)
    # create iterator once per epoch
    val_iter = iter(val_loader)
    
    def val_batch_wrapper(bi):
        try:
            batch = next(val_iter)
        except StopIteration:
            # Fallback if length calculation slightly off
            batch = next(iter(val_loader))
            
        x_seq, y_pol, y_next, y_traj = batch
        x_seq = x_seq.to(device, non_blocking=True)
        y_pol = y_pol.to(device, non_blocking=True)
        # Dummy regime (not used in this training phase)
        r = torch.zeros(x_seq.size(0), dtype=torch.long, device=device)
        return x_seq, y_pol, r

    head_losses = compute_val_head_losses(
        model=model,
        get_batch_fn=val_batch_wrapper,
        n_batches=len(val_loader),
        device=device,
        amp_dtype=torch.float16 # Standard AMP
    )
    
    # 2. Log per-head scalars
    if writer is not None:
        for head_name, h_loss in head_losses.items():
            writer.add_scalar(f'HeadLoss/{head_name}', h_loss, epoch+1)
            
    # 3. Rich Image Logging (Scatter plots & Trajectories)
    # Run every epoch or every few epochs
    if writer is not None:
        try:
            # Get sample predictions
            samples = sample_predictions(
                model=model,
                get_batch_fn=lambda bi: next(iter(val_loader)),
                device=device,
                amp_dtype=torch.float16,
                n_samples=32
            )
            
            # A. Predicted vs Actual Scatter Plots
            for i, head_name in enumerate(MAIN_HEADS):
                fig, ax = plt.subplots(1, 1, figsize=(6, 6))
                p = samples['preds'][:, i]
                t = samples['targets'][:, i]
                
                ax.scatter(t, p, alpha=0.6, s=50, c='blue', edgecolors='black')
                vmin, vmax = min(p.min(), t.min()), max(p.max(), t.max())
                margin = (vmax - vmin) * 0.1 + 0.01
                ax.plot([vmin-margin, vmax+margin], [vmin-margin, vmax+margin], 'k--', alpha=0.5)
                
                mae = np.mean(np.abs(p - t))
                corr = np.corrcoef(p, t)[0, 1] if np.std(p) > 1e-6 else 0
                ax.set_title(f'{head_name}\nMAE={mae:.4f} | r={corr:.3f}')
                ax.set_xlabel('Actual')
                ax.set_ylabel('Predicted')
                ax.grid(True, alpha=0.3)
                fig.tight_layout()
                
                # Convert to image
                buf = io.BytesIO()
                fig.savefig(buf, format='png', dpi=80)
                buf.seek(0)
                img = Image.open(buf)
                img_array = np.array(img)
                plt.close(fig)
                
                writer.add_image(f'Predictions/{head_name}', img_array, epoch+1, dataformats='HWC')
            
            # B. Horizon Trajectory (45-Day Forecast)
            if samples.get('forecast_data') is not None:
                forecast = samples['forecast_data'][0]  # Sample 0
                num_days = forecast.shape[0]
                days = np.arange(num_days)
                
                fig, ax = plt.subplots(1, 1, figsize=(10, 6))
                # forecast components: [close, high, low, vol]
                ax.plot(days, forecast[:, 0], 'b-', label='Expected Close', linewidth=2)
                ax.fill_between(days, forecast[:, 2], forecast[:, 1], color='blue', alpha=0.2, label='High/Low Envelope')
                
                ax.set_title(f'HorizonForecaster: 45-Day Trajectory (Epoch {epoch+1})')
                ax.set_xlabel('Days from Now')
                ax.set_ylabel('Normalized Price Change')
                ax.legend()
                ax.grid(True, alpha=0.3)
                buf = io.BytesIO()
                fig.savefig(buf, format='png', dpi=100)
                buf.seek(0)
                img = Image.open(buf)
                img_array = np.array(img)
                plt.close(fig)
                writer.add_image('Horizon/Trajectory', img_array, epoch+1, dataformats='HWC')

            # Force flush to disk so Colab sees it immediately
            writer.flush()
            print(f"   [TensorBoard] ðŸ“¸ Logged validation images (Scatter + Horizon) to {log_dir}")

        except Exception as e:
            print(f"[TensorBoard] Image logging error: {e}")

    # Save Checkpoint with Metadata
    save_path = f"condor_brain_retrain_e{epoch+1}.pth"
    
    state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
    
    checkpoint = {
        "state_dict": state_dict,
        "version": VERSION_V21,
        "feature_cols": FEATURE_COLS,
        "input_dim": INPUT_DIM,
        "median": median.astype(np.float32),
        "mad": mad.astype(np.float32),
        "seq_len": SEQ_LEN,
        "use_diffusion": True,
        "diffusion_steps": 50
    }
    
    torch.save(checkpoint, save_path)
    print(f"      ðŸ’¾ Saved: {save_path}")

print("âœ… Retraining Complete.")
